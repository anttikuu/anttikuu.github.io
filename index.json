[{"authors":["admin"],"categories":null,"content":"Hi! I\u0026rsquo;m working as a postdoc at Aalto University Acoustics Lab. My research concerns human perception of speech and music in reverberant spaces, including such aspects as listening effort and the effects of hearing impairment.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://anttikuusinen.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi! I\u0026rsquo;m working as a postdoc at Aalto University Acoustics Lab. My research concerns human perception of speech and music in reverberant spaces, including such aspects as listening effort and the effects of hearing impairment.","tags":null,"title":"Antti Kuusinen","type":"authors"},{"authors":null,"categories":["R","Data analysis"],"content":" . -- TRYING TO UPDATE\nIn this post I will briefly discuss and illustrate the analysis of perceptual matching experiment on concert hall acoustics. How the sound samples / auralizations of concert hall acoustics in this experiment were made is a topic of another time.\n(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)\nIntroduction The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni. over the past decade or so. There are always some ideas left in old notebooks that did not get studied at the time they emerged and this perceptual matching experiment is one of those.\nThe premise and the guiding idea in many of our previous works has been the ability to discern, evaluate and analyse even the tinyest differences and the different flavors and nuances of concert hall acoustics. Thus, previously most of our perceptual studies have been conducted so that the differences between different halls would be as easy to perceive as possible. One of the best ways to reveal small differences in room acoustics is to play back the music (signal) continuously so that when the sample is switched to another hall, only the surrounding acoustics changes while the music goes on and on. However, the acoustic “teleportation” is only possible at the lab, and in natural environments we may perceptually evaluate the acoustical characteristics and differences between spaces only by physically moving from one place to another, for instance, from concert hall to concert hall. And aside from research projects, seldom exactly the same sounds are listened to, but commonly our perceptions and assessments of room acoustical qualities are based on the experience of different sounds in the different rooms.\nSo here we simply asked that are people able to detect and match concert halls when they are listening to same or different excitation signals?\nSo, in this experiment the listener is presented with a sound that is auralized to a particular concert hall (reference), and then she/he needs to find the same concert hall among four alternatives with different or same sound auralized in those halls. Listener is also presented with the same sounds to enable comparison of performance between the different and same sounds. The experiment was implemented in Matlab. Here is a figure of the GUI:\nThe experiment is constructed with the following variables:\n Four concert halls coded as “AC”, “BP”, “CP”, “MH”.  Each one acts as the reference in each sound case  MUSIC: Full orchestra  Two excerpts (7 s) from the same piece by Beethoven -\u0026gt; “BEE1” and “BEE2” Combinations:  same sounds: BEE1 vs BEE1; BEE2 vs BEE2 different sounds: BEE1 vs BEE2; BEE2 vs BEE1 each hall acts as a ref -\u0026gt; 4*4 = 16 trials   INSTR: Single violin  Single sounds and short passages (\u0026lt; 6 s) Combinations:  same: i.e., Violin 1 vs Violin 1 (8 trials) different: i.e., Violin 1 vs Violin 2 (8 trials) Note that these are single intrument sounds and it does not make much sense to do Violin 2 vs Violin 2, as it would be basically the same as Violin 1 vs Violin 1.   NOTE: Matching is done only within MUSIC or INSTR sounds, meaning that Beethoven is not compared to Violin sounds or vice versa.  Finally, in order to make this experiment a little more robust,\nWe would have liked to include one repetition of each of the trials, but because this experiment was scheduled to be run in a single afternoon with approx. 15 people, the experiment should not take more than 30 min to complete and it was necesssary to sacrifice the repetition in order to reduce the final lenght of experiment.\nFor the uninitialized: Reading the data from .csv files in R In this experiment, I chose to set up Matlab to output the results as .csv files for each listener and then move to R for the data analysis.\nLet’s first have a peak at the data of a single individual:\n# READ SINGLE CSV FILE test \u0026lt;- read.csv2(\u0026#39;S01.csv\u0026#39;, sep = \u0026quot;,\u0026quot;) head(test) ## SUB_ID ORD CASE_ID SOUNDTYPE SOUNDNUM SAMEDIFF MUS_REF MUS_COM REF A B C ## 1 S01 1 1 MUSIC 0 same BEE1 BEE1 AC CP AC MH ## 2 S01 2 2 MUSIC 0 same BEE1 BEE1 BP BP AC CP ## 3 S01 3 5 MUSIC 0 diff BEE1 BEE2 AC BP AC MH ## 4 S01 4 23 INSTR 7 diff viulut viulut2 MH AC BP CP ## 5 S01 5 3 MUSIC 0 same BEE1 BEE1 MH MH BP CP ## 6 S01 6 14 MUSIC 0 same BEE2 BEE2 BP AC BP CP ## D ANS CORRECT TIME COMPLETED REP ## 1 BP BP 0 14-Jan-2020 13:34:28 1 1 ## 2 MH \u0026lt;NA\u0026gt; 0 13-Jan-2020 15:27:52 0 1 ## 3 CP \u0026lt;NA\u0026gt; 0 13-Jan-2020 15:27:52 0 1 ## 4 MH \u0026lt;NA\u0026gt; 0 13-Jan-2020 15:27:52 0 1 ## 5 AC \u0026lt;NA\u0026gt; 0 13-Jan-2020 15:27:52 0 1 ## 6 MH \u0026lt;NA\u0026gt; 0 13-Jan-2020 15:27:52 0 1 So this basically illustrates also the design matrix of the experiment; the different columns hold the variables, for instance, MUS_REF indicates the sound of the REF and MUS_COM that of the comparison samples. SAMEDIFF column indicates whether the MUS_REF and MUS_COM were the same or different sounds. REF and A-D shows the hall names behind the GUI buttond and the column ANS indicates the chosen response and CORRECT indicates whether the answer was correct (1) or not (0).\n(No need to worry about some of the columns, for instance, REP does not mean anything here as there were no repetitions.)\nThen a set of individual .csv -files can be read and bind to single data.frame as follows (For illustration purposes, now here is only 4 individual .csv files):\n# LIST .csv files csvfiles = list.files(pattern = \u0026#39;.csv\u0026#39;) # Make an empty list chm_data \u0026lt;- list(); # Populate the list entries from the .csv-files for (rfile in 1:length(csvfiles)){ filename \u0026lt;- csvfiles[rfile] chm_data[[rfile]] \u0026lt;- read.csv2(filename, sep = \u0026quot;,\u0026quot;) } # ROW BIND THE LIST ELEMENTS: \u0026quot;UNLIST\u0026quot; (the unlist() -function outputs an atomic vector of all elements and does not work here) chm_df \u0026lt;- do.call(rbind, chm_data) # # CAST AS DATA.FRAME: chm_df \u0026lt;- data.frame(chm_df) # not run here #summary(chm_df) # SAVE AS TXT: #write.table(chm_df, file = \u0026#39;LONGTABLE2.txt\u0026#39;, sep = \u0026#39;,\u0026#39;, quote = F) # OR AS .RData: #save(chm_df, file = \u0026quot;LONGTABLE2.RData\u0026quot;) # CHECK data str(chm_df) ## \u0026#39;data.frame\u0026#39;: 192 obs. of 18 variables: ## $ SUB_ID : chr \u0026quot;S01\u0026quot; \u0026quot;S01\u0026quot; \u0026quot;S01\u0026quot; \u0026quot;S01\u0026quot; ... ## $ ORD : int 1 2 3 4 5 6 7 8 9 10 ... ## $ CASE_ID : int 1 2 5 23 3 14 4 24 10 8 ... ## $ SOUNDTYPE: chr \u0026quot;MUSIC\u0026quot; \u0026quot;MUSIC\u0026quot; \u0026quot;MUSIC\u0026quot; \u0026quot;INSTR\u0026quot; ... ## $ SOUNDNUM : int 0 0 0 7 0 0 0 8 0 0 ... ## $ SAMEDIFF : chr \u0026quot;same\u0026quot; \u0026quot;same\u0026quot; \u0026quot;diff\u0026quot; \u0026quot;diff\u0026quot; ... ## $ MUS_REF : chr \u0026quot;BEE1\u0026quot; \u0026quot;BEE1\u0026quot; \u0026quot;BEE1\u0026quot; \u0026quot;viulut\u0026quot; ... ## $ MUS_COM : chr \u0026quot;BEE1\u0026quot; \u0026quot;BEE1\u0026quot; \u0026quot;BEE2\u0026quot; \u0026quot;viulut2\u0026quot; ... ## $ REF : chr \u0026quot;AC\u0026quot; \u0026quot;BP\u0026quot; \u0026quot;AC\u0026quot; \u0026quot;MH\u0026quot; ... ## $ A : chr \u0026quot;CP\u0026quot; \u0026quot;BP\u0026quot; \u0026quot;BP\u0026quot; \u0026quot;AC\u0026quot; ... ## $ B : chr \u0026quot;AC\u0026quot; \u0026quot;AC\u0026quot; \u0026quot;AC\u0026quot; \u0026quot;BP\u0026quot; ... ## $ C : chr \u0026quot;MH\u0026quot; \u0026quot;CP\u0026quot; \u0026quot;MH\u0026quot; \u0026quot;CP\u0026quot; ... ## $ D : chr \u0026quot;BP\u0026quot; \u0026quot;MH\u0026quot; \u0026quot;CP\u0026quot; \u0026quot;MH\u0026quot; ... ## $ ANS : chr \u0026quot;BP\u0026quot; NA NA NA ... ## $ CORRECT : int 0 0 0 0 0 0 0 0 0 0 ... ## $ TIME : chr \u0026quot;14-Jan-2020 13:34:28\u0026quot; \u0026quot;13-Jan-2020 15:27:52\u0026quot; \u0026quot;13-Jan-2020 15:27:52\u0026quot; \u0026quot;13-Jan-2020 15:27:52\u0026quot; ... ## $ COMPLETED: int 1 0 0 0 0 0 0 0 0 0 ... ## $ REP : int 1 1 1 1 1 1 1 1 1 1 ... The function str() summarises the variables and their types and it is always worthwhile to check that there no some funny business going on.\nNow, instead of using only 4 individuals, I have simulated random results for 20 individuals, in the hope to better reflect the real data that we are aiming at.\nThese simulated results have been saved in “SIMDATA.txt”, which is in the same format as the data.frame generated above.\nThis data set will be used in the next steps of this presentation.\n# First remove variables from workspace rm(list = ls()) # and read the data.table: simdata \u0026lt;- read.table(\u0026quot;SIMDATA.txt\u0026quot;, sep = \u0026quot;,\u0026quot;, header = T,stringsAsFactors = T) head(simdata) ## SUB_ID ORD CASE_ID SOUNDTYPE SOUNDNUM SAMEDIFF MUS_REF MUS_COM REF A B C ## 1 S1 1 21 INSTR 5 diff viulut viulut2 AC AC BP MH ## 2 S1 2 31 INSTR 15 diff viulut viulut2 MH AC MH CP ## 3 S1 3 4 MUSIC 0 same BEE1 BEE1 CP MH AC BP ## 4 S1 4 3 MUSIC 0 same BEE1 BEE1 MH MH AC BP ## 5 S1 5 7 MUSIC 0 diff BEE1 BEE2 MH AC MH BP ## 6 S1 6 13 MUSIC 0 same BEE2 BEE2 AC CP AC MH ## D ANS CORRECT TIME COMPLETED REP ## 1 CP MH 0 18-Jan-2020 21:09:13 0 1 ## 2 BP CP 0 18-Jan-2020 21:09:13 0 1 ## 3 CP MH 0 18-Jan-2020 21:09:12 0 0 ## 4 CP AC 0 18-Jan-2020 21:09:12 0 0 ## 5 CP BP 0 18-Jan-2020 21:09:12 0 0 ## 6 BP CP 0 18-Jan-2020 21:09:12 0 1   Preliminaries Now that we have our raw dataset ready for analysis, let’s first think little about the objectives of the next steps and what we would like our data to tell us.\nSo we are interested in the following main question:\nAre listeners able to match the samples based on the room acoustics of the concert halls and are there differences in performances when they do this with the same and different excitation signals?\nThis question can then be broken down to different levels:\n Overall results (i.e., all same and all different) Results between MUSIC (Beethoven) and INSTR (Violin) Possible differences between concert halls (i.e.,are some concert halls confused between each other more than others?) Results of each individual  In the skinning of any dataset, a good first step is to try to plot the results in some way.\nHere we will simply calculate the percentages of the correct answer per each individual and then make a boxplot of the results with ggplot. (Boxplot depicts the data with the median and the interquartile range (IQR) (Q1 (25 %) - Q3 (75%)) with whiskers extending to Q1-1.5*IQR and Q3+1.5*IQR.)\nCalculate the percentages and make a corresponding data.frame:\nSUBS \u0026lt;- levels(simdata$SUB_ID) dataf \u0026lt;- c() ids \u0026lt;- c() for(s in levels(simdata$SUB_ID)) { sdata \u0026lt;- subset(simdata, SUB_ID == s) # EXTRACT SUBJECT DATA ids \u0026lt;- c(ids, s) # KEEP TRACK OF SUB ID musvec \u0026lt;- c() # init sdtemp \u0026lt;- c() # init for(ss in levels(sdata$SAMEDIFF)) { samediffdata \u0026lt;- subset(sdata, SAMEDIFF == ss) # EXTRACT same or diff DATA # GET THE PERCENTAGE OF CORRECT ANS sdtemp[ss] \u0026lt;- as.numeric(round(sum(samediffdata$CORRECT)/length(samediffdata$CORRECT), digits = 2)) mustemp \u0026lt;- c() # init for(sss in levels(samediffdata$MUS_REF)) { musdata \u0026lt;- subset(samediffdata, MUS_REF == sss) # ECTRACT DATA by sound mustemp[sss] \u0026lt;- round(sum(musdata$CORRECT)/length(musdata$CORRECT), digits = 2) # GET THE PERCENTAGE OF CORRECT ANS names(mustemp)[names(mustemp) == sss] \u0026lt;- c(paste(musdata$MUS_REF[[1]],\u0026#39;-\u0026#39;,musdata$MUS_COM[[1]], sep = \u0026quot;\u0026quot;)) # KEEP TRACK OF THE COMPARISONS } musvec \u0026lt;- c(musvec, mustemp) # POPULATE } sdvec \u0026lt;- c(sdtemp, musvec) # POPULATE dataf \u0026lt;- rbind(dataf, sdvec) # POPULATE } # SOME CLEANING UP AND FINALISING OF THE DATAFRAME: SAMEDIFF \u0026lt;- data.frame(matrix(as.numeric(dataf[,1:8]),nrow = length(SUBS))) colnames(SAMEDIFF) \u0026lt;- c(names(sdtemp),names(musvec)) SAMEDIFF \u0026lt;- cbind(ids, SAMEDIFF) colnames(SAMEDIFF)[1] \u0026lt;- \u0026#39;ID\u0026#39; head(SAMEDIFF) # show first 6 rows ## ID diff same BEE1-BEE2 BEE2-BEE1 viulut-viulut2 BEE1-BEE1 BEE2-BEE2 ## 1 S1 0.19 0.19 0.50 0.25 0.00 0.25 0.00 ## 2 S10 0.25 0.25 0.50 0.25 0.12 0.00 0.25 ## 3 S11 0.31 0.38 0.00 0.25 0.50 0.50 0.25 ## 4 S12 0.19 0.19 0.00 0.50 0.12 0.00 0.00 ## 5 S13 0.44 0.25 0.75 0.25 0.38 0.25 0.50 ## 6 S14 0.31 0.25 0.00 0.50 0.38 0.00 0.00 ## viulut-viulut ## 1 0.25 ## 2 0.38 ## 3 0.38 ## 4 0.38 ## 5 0.12 ## 6 0.50 Then make a “long”-format table with melt()-function and plot the results with ggplot.\n#### BOXPLOTS: OVERALL SAME vs DIFF library(reshape2) # melt()-function # melt the data into \u0026quot;long\u0026quot;-format for ggplotting m0 = melt(SAMEDIFF[,c(\u0026#39;same\u0026#39;,\u0026#39;diff\u0026#39;)],measure.vars = c(\u0026#39;same\u0026#39;, \u0026#39;diff\u0026#39;), variable.name = \u0026#39;G\u0026#39;) library(ggplot2) g1 \u0026lt;- ggplot(data = m0, aes(y = value, x = G)) + theme_bw() + geom_boxplot(outlier.shape = NA) + geom_point(aes(y = value, x = G, shape = G), position = position_jitter(width = 0.2), size = 1, alpha = 1, show.legend = F, color = \u0026#39;grey\u0026#39;) + coord_cartesian(ylim = c(0,1)) + labs(title = \u0026#39;\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;Percentage of correct answers\u0026#39;, size = 0.1, color = \u0026#39;black\u0026#39;) + theme(axis.title.y = element_text(size = 8)) + ggtitle(\u0026#39;Overall\u0026#39;) # FOR SAVING THE PLOT: (not executed) #ggsave(filename = \u0026#39;SRT_basic_boxplot.eps\u0026#39;, g1, width = 10, height = 6, units = \u0026#39;cm\u0026#39;) #### BOXPLOTS: MUSIC (Beethoven) vs INSTR (Violin) m1 = melt(SAMEDIFF[,colnames(SAMEDIFF)[4:9]], measure.vars = colnames(SAMEDIFF)[4:9], variable.name = \u0026#39;musiclist\u0026#39;) samedifflist \u0026lt;- c(rep(\u0026#39;diff\u0026#39;, 60),rep(\u0026#39;same\u0026#39;, 60)) muslist \u0026lt;- c(rep(\u0026#39;Beethoven\u0026#39;, 40), rep(\u0026#39;Violin\u0026#39;, 20), rep(\u0026#39;Beethoven\u0026#39;, 40), rep(\u0026#39;Violin\u0026#39;, 20)) m1 \u0026lt;- cbind(m1,samedifflist, muslist) g2 \u0026lt;- ggplot(data = m1, aes(y = value, x = samedifflist,colour = muslist),show.legend = F) + theme_bw() + geom_boxplot(outlier.shape = NA,show.legend = F)+ geom_point(aes(y = value, colour = muslist), position = position_jitter(width = 0.2), size = 0.5, alpha = 0.5, show.legend = F, color = \u0026#39;grey\u0026#39;) + facet_grid(.~muslist) + coord_cartesian(ylim = c(0,1)) + labs(title = \u0026#39;\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;Percentage of correct answers\u0026#39;, size = 0.1, color = \u0026#39;black\u0026#39;) + theme(axis.title.y = element_text(size = 8)) + ggtitle(\u0026#39;Per music\u0026#39;) library(gridExtra) # for grid.arrange() grid.arrange(g1,g2,nrow = 1) Now we have the first view of our data. Note that as expected with randomised data the median percentages of correct answers set nicely on the change level of 1/4 = 25 %.\nWith another dataset one would be inclined to run a test, such as Kruskal-Wallis rank sum test (non-parametric one way anova by ranks) to test whether the distributions of the percentages of correct answers differ between the same and difference cases. Using the m1-data from above, one may run this e.g., by kruskal.test(x=m1$value, g=m1$samedifflist).\nNow, besides just looking at the correct answers we can look at the actual perceived halls (ANS column) versus the true halls (REF column).\nThis way the experiment now presents itself as a classical multiclass classification problem, where the listeners makes class “predictions” based on one’s perceptions.\nTherefore, these results are perhaps best presented and analysed in the spirit of machine learning and treated with the tools, concepts and metrics developed for the classification tasks and for the analysis of confusion matrices.\nOnto the confusion matrices Confusion matrices can be exctracted in quite a straightforward manner from our data:\nhalls \u0026lt;- levels(simdata$REF) # OVERALL SAME: hallmat1 \u0026lt;- matrix(0,nrow = 4, ncol = 4); rownames(hallmat1) \u0026lt;- halls colnames(hallmat1) \u0026lt;- halls samedata \u0026lt;- subset(simdata, SAMEDIFF == \u0026#39;same\u0026#39;) for(i in 1:nrow(samedata)){ hallmat1[as.character(samedata$REF[i]), as.character(samedata$ANS[i])] \u0026lt;- hallmat1[as.character(samedata$REF[i]), as.character(samedata$ANS[i])] + 1 } # OVERALL DIFF: hallmat2 \u0026lt;- matrix(0,nrow = 4, ncol = 4); rownames(hallmat2) \u0026lt;- halls colnames(hallmat2) \u0026lt;- halls diffdata \u0026lt;- subset(simdata, SAMEDIFF == \u0026#39;diff\u0026#39;) for(i in 1:nrow(samedata)){ hallmat2[as.character(diffdata$REF[i]),as.character(diffdata$ANS[i])] \u0026lt;- hallmat2[as.character(diffdata$REF[i]),as.character(diffdata$ANS[i])] + 1 } # PER MUSIC, SAME samemats \u0026lt;- list() for(s in levels(simdata$MUS_REF)) { hallmat \u0026lt;- matrix(0,nrow = 4, ncol = 4); rownames(hallmat) \u0026lt;- halls colnames(hallmat) \u0026lt;- halls data \u0026lt;- subset(simdata, SAMEDIFF == \u0026#39;same\u0026#39; \u0026amp; MUS_REF == as.character(s)) for(i in 1:nrow(data)){ hallmat[as.character(data$REF[i]),as.character(data$ANS[i])] \u0026lt;- hallmat[as.character(data$REF[i]), as.character(data$ANS[i])] + 1 } samemats[[as.character(s)]] \u0026lt;- hallmat } # PER MUSIC, DIFF diffmats \u0026lt;- list() for(s in levels(simdata$MUS_REF)) { hallmat \u0026lt;- matrix(0,nrow = 4, ncol = 4); rownames(hallmat) \u0026lt;- halls colnames(hallmat) \u0026lt;- halls data \u0026lt;- subset(simdata, SAMEDIFF == \u0026#39;diff\u0026#39; \u0026amp; MUS_REF == as.character(s)) for(i in 1:nrow(data)){ hallmat[as.character(data$REF[i]),as.character(data$ANS[i])] \u0026lt;- hallmat[as.character(data$REF[i]), as.character(data$ANS[i])] + 1 } diffmats[[as.character(s)]] \u0026lt;- hallmat } # PER MUSIC, MUSIC VS MUSIC: musmatsS \u0026lt;- list() for(s in levels(simdata$MUS_REF)) { musmatsSS \u0026lt;- list() data \u0026lt;- subset(simdata, MUS_REF == as.character(s)) for(ss in levels(simdata$MUS_COM)) { data2 \u0026lt;- subset(data, MUS_COM == as.character(ss)) hallmat \u0026lt;- matrix(0,nrow = 4, ncol = 4); rownames(hallmat) \u0026lt;- halls colnames(hallmat) \u0026lt;- halls if(nrow(data2) \u0026gt; 0) { for(i in 1:nrow(data2)){ hallmat[as.character(data2$REF[i]),as.character(data2$ANS[i])] \u0026lt;- hallmat[as.character(data2$REF[i]), as.character(data2$ANS[i])] + 1 } } musmatsSS[[as.character(ss)]] \u0026lt;- hallmat } musmatsS[[as.character(s)]] \u0026lt;- musmatsSS } # Let\u0026#39;s see what we have: hallmat1 # overall same ## AC BP CP MH ## AC 23 16 18 23 ## BP 15 26 23 16 ## CP 17 24 19 20 ## MH 21 20 24 15 #hallmat2 # overall diff (not shown) #samemats # per music, same (not shown) #diffmats # per music, diff (not shown) #musmatsS # per music vs music (not shown)  Analysis with caret-package Now that we have our set of main confusion matrices ready (we won’t be looking at individual level performance here), there a various R packages that can be used for the analysis. Here, we will be using the caret -package, and as an example, we will analyse only a single matrix.\nlibrary(caret) # caret-pkg ## Loading required package: lattice library(e1071) # this is required in rstudio # OVERALL SAME: confumat \u0026lt;- hallmat1 # assign to confumat # For using caret confusionMatrix-function we need to spell out the reference and response table in this way: Reference \u0026lt;- factor(rep(halls, times = c(sum(confumat[1,]),sum(confumat[2,]),sum(confumat[3,]),sum(confumat[4,]))), levels = halls) Response \u0026lt;- factor( c( rep(halls, times = c(confumat[1,1], confumat[1,2], confumat[1,3], confumat[1,4])), rep(halls, times = c(confumat[2,1], confumat[2,2], confumat[2,3], confumat[2,4])), rep(halls, times = c(confumat[3,1], confumat[3,2], confumat[3,3], confumat[3,4])), rep(halls, times = c(confumat[4,1], confumat[4,2], confumat[4,3], confumat[4,4]))), levels = halls ) # OBS: caret::confusionMatrix -function will need the data so that responses (predictions) are in the order table(Response,Reference), so responses in rows and references in cols. (Opposite of our experiment) xtab \u0026lt;- table(Response,Reference) And now we are ready to run the analysis function from the caret-pckg:\n#### ANALYSIS: confusionMatrix(xtab, mode = \u0026quot;everything\u0026quot;) ## Confusion Matrix and Statistics ## ## Reference ## Response AC BP CP MH ## AC 23 15 17 21 ## BP 16 26 24 20 ## CP 18 23 19 24 ## MH 23 16 20 15 ## ## Overall Statistics ## ## Accuracy : 0.2594 ## 95% CI : (0.2122, 0.3111) ## No Information Rate : 0.25 ## P-Value [Acc \u0026gt; NIR] : 0.3698 ## ## Kappa : 0.0125 ## ## Mcnemar\u0026#39;s Test P-Value : 0.9863 ## ## Statistics by Class: ## ## Class: AC Class: BP Class: CP Class: MH ## Sensitivity 0.28750 0.32500 0.23750 0.18750 ## Specificity 0.77917 0.75000 0.72917 0.75417 ## Pos Pred Value 0.30263 0.30233 0.22619 0.20270 ## Neg Pred Value 0.76639 0.76923 0.74153 0.73577 ## Precision 0.30263 0.30233 0.22619 0.20270 ## Recall 0.28750 0.32500 0.23750 0.18750 ## F1 0.29487 0.31325 0.23171 0.19481 ## Prevalence 0.25000 0.25000 0.25000 0.25000 ## Detection Rate 0.07187 0.08125 0.05937 0.04688 ## Detection Prevalence 0.23750 0.26875 0.26250 0.23125 ## Balanced Accuracy 0.53333 0.53750 0.48333 0.47083 As shown, the confusionMatrix()-function outputs a set of performance metrics, and information on these metrics can be found from the wikipedia page. Also give a look here and here for some more tutorials on multiclass classification\nHere is a very short summary about the terminology and the derivations:\n condition positive (P) : the number of real positive cases in the data condition negative (N): the number of real negative cases in the data true positive (TP), items that are correctly classified, i.e., “hit” true negative (TN), items that are correctly classified as not belonging to the class, i.e. correct rejection false positive (FP), items that are incorrectly perceived to belong to the class, i.e., false alarm (Type I error) false negative (FN), items that are not perceived as belonging to the class but should have been; i.e., “miss” (Type II error)  The main metrics that may be interested in:\n Accuracy: The overall accuracy of the prediction (TP + TN) / P + N\n Recall; sensitivity; hit rate; true positive rate : the proportion of correct answers to total answers TP / P = TP / (TP + FN)\n Precision is the proportion of predictions that are correct from all “positive” predictions of that class TP / (TP + FP)\n  Recall and precision (and other metrics) can be calculated the following table that is generated separately for each hall :\n   Reference     Prediction Hall 1 Hall 234  Hall 1 A (TP) B (FP)  Hall 234 C (FN) D (TN)    And for your reference (from caret-pckg documentation)\n Sensitivity = A/(A+C) Specificity = D/(B+D) Prevalence = (A+C)/(A+B+C+D) Positive Predictive Value (PPV) (same as Precision) = (sensitivity * prevalence)/((sensitivity*prevalence) + ((1-specificity)*(1-prevalence))) Negative Preditive Value (NPV) = (specificity * (1-prevalence))/(((1-sensitivity)prevalence) + ((specificity)(1-prevalence))) Detection Rate = A/(A+B+C+D) Detection Prevalence = (A+B)/(A+B+C+D) Balanced Accuracy = (sensitivity+specificity)/2  The output includes Cohen’s Kappa statistic which measures the agreement between the predictions and the reference counts and it ranges from 0 to 1, so that, close to zero values indicate that the performance is not better than change level.\nIt is interesting that also P-value of McNemar’s test is included. McNemar’s test indicates whether the marginal frequecies in the confusion matrix are equal (null hypothesis) or not (alternative hypothesis). This test requires matched pairs of observations, and I do not fully understand what is the meaning here.\nHowever, McNemar’s test is actually quite useful in many cases when one can form matched pairs from the data, and for our current analysis, there is actually another use of McNemar’s test illustrated next.\nWe can generate a contingengy table between the “same” and “different” sounds from our data by forming matched pairs where the same subject has made task with a particular sound and a particular concert hall as the reference sample. The following script illustrates what I mean:\n# McNemar\u0026#39;s test; create contingengy table between same and different by pairing the answers # case by case A \u0026lt;- 0 # number of correct answers on both same and different B \u0026lt;- 0 # number of answers where same is correct and different is incorrect C \u0026lt;- 0 # number of answers where same is incorrect and different is correct D \u0026lt;- 0 # number of incorrect answers on both same and different musref \u0026lt;- levels(simdata$MUS_REF) halls \u0026lt;- levels(simdata$REF) subs \u0026lt;- levels(simdata$SUB_ID) for(m in musref){ for (h in halls){ sametemp \u0026lt;- subset(simdata, REF == h \u0026amp; MUS_REF == m \u0026amp; SAMEDIFF == \u0026#39;same\u0026#39;) difftemp \u0026lt;- subset(simdata, REF == h \u0026amp; MUS_REF == m \u0026amp; SAMEDIFF == \u0026#39;diff\u0026#39;) for (s in subs){ if (sametemp[sametemp$SUB_ID == s,]$CORRECT == 1 \u0026amp;\u0026amp; difftemp[difftemp$SUB_ID == s,]$CORRECT == 1) { A \u0026lt;- A + 1; } if (sametemp[sametemp$SUB_ID == s,]$CORRECT == 1 \u0026amp;\u0026amp; difftemp[difftemp$SUB_ID == s,]$CORRECT == 0){ B \u0026lt;- B + 1 } if (sametemp[sametemp$SUB_ID == s,]$CORRECT == 0 \u0026amp;\u0026amp; difftemp[difftemp$SUB_ID == s,]$CORRECT == 1){ C \u0026lt;- C + 1 } if (sametemp[sametemp$SUB_ID == s,]$CORRECT == 0 \u0026amp;\u0026amp; difftemp[difftemp$SUB_ID == s,]$CORRECT == 0){ D \u0026lt;- D + 1 } } } } # CHECK THE RESULTS: c(A, B, C, D) ## [1] 13 45 49 133 # Perform the McNemar\u0026#39;s test Matching \u0026lt;- matrix(c(A, C, B, D), nrow = 2, dimnames = list(\u0026quot;Same\u0026quot; = c(\u0026quot;Correct\u0026quot;, \u0026quot;Incorrect\u0026quot;), \u0026quot;Diff\u0026quot; = c(\u0026quot;Correct\u0026quot;, \u0026quot;Incorrect\u0026quot;))) Matching ## Diff ## Same Correct Incorrect ## Correct 13 45 ## Incorrect 49 133 mcnemar.test(Matching) ## ## McNemar\u0026#39;s Chi-squared test with continuity correction ## ## data: Matching ## McNemar\u0026#39;s chi-squared = 0.095745, df = 1, p-value = 0.757 In this way, McNemar’s test provides as a nice direct test statistic in terms of our question whether there is a difference between between our “same” and “different” cases.\nAll together the analysis of this matching data (when it is also looked at different levels) seems now to be quite exhaustive.\n…\nFinally, in order to have some more figures to accompany the analysis here is an example of how to illustrate the results by plotting the confusion matrices with ggplot2. (in the following, xtab2 was generated the same way as xtab.)\n### PLOTTING WITH GGPLOT # SAME plot normvalue \u0026lt;- sum(hallmat1[1,]) # normalize melted_xtab \u0026lt;- melt(round(xtab1/normvalue,2)) gsame \u0026lt;- ggplot(data = melted_xtab, aes(x=Response, y=Reference, fill=value),show.legend = F) + geom_tile(show.legend = F) + theme_minimal() + geom_text(aes(Response, Reference, label = value), color = \u0026quot;black\u0026quot;, size = 3,show.legend = F) + ggtitle(\u0026#39;Same\u0026#39;) # DIFF Plot normvalue \u0026lt;- sum(hallmat2[1,]) melted_xtab \u0026lt;- melt(round(xtab2/normvalue,2)) gdiff \u0026lt;- ggplot(data = melted_xtab, aes(x=Response, y=Reference, fill=value),show.legend = F) + geom_tile(show.legend = F) + theme_minimal() + geom_text(aes(Response, Reference, label = value), color = \u0026quot;black\u0026quot;, size = 3,show.legend = F) +ggtitle(\u0026#39;Different\u0026#39;) #gdiff grid.arrange(gsame,gdiff,nrow=1) So there we are! And we will stop for now with this simulated dataset.\n Next steps.. Next steps would be to analyse the dataset in detail, investigate the differences between same and different excitation signals, evaluate whether there are some particular patterns emerging between the halls, and finally to provide an answer to the original question(s) about the human ability to match concert halls when listening to the same and different excitation signals?\nIf all goes as planned you’ll find the answer in a proper article sometime in the future and I’ll then link it here.\nLeave a comment:\n   ","date":1579651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579651200,"objectID":"41f0f7c81fe627357878b3b6704af408","permalink":"https://anttikuusinen.github.io/post/2020-22-01-chm/2020-22-01-chm/","publishdate":"2020-01-22T00:00:00Z","relpermalink":"/post/2020-22-01-chm/2020-22-01-chm/","section":"post","summary":". -- TRYING TO UPDATE\nIn this post I will briefly discuss and illustrate the analysis of perceptual matching experiment on concert hall acoustics. How the sound samples / auralizations of concert hall acoustics in this experiment were made is a topic of another time.\n(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)\nIntroduction The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni.","tags":["R Markdown","data preprocessing","confusion matrix","classification"],"title":"On the data analysis of perceptual matching of concert hall acoustics","type":"post"},{"authors":null,"categories":["R","Data analysis"],"content":" . -- TRYING TO UPDATE previous post.\n(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)\nIntroduction The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni. over the past decade or so. There are always some ideas left in old notebooks that did not get studied at the time they emerged and this perceptual matching experiment is one of those.\nThe premise and the guiding idea in many of our previous works has been the ability to discern, evaluate and analyse even the tinyest differences and the different flavors and nuances of concert hall acoustics. Thus, previously most of our perceptual studies have been conducted so that the differences between different halls would be as easy to perceive as possible. One of the best ways to reveal small differences in room acoustics is to play back the music (signal) continuously so that when the sample is switched to another hall, only the surrounding acoustics changes while the music goes on and on. However, the acoustic “teleportation” is only possible at the lab, and in natural environments we may perceptually evaluate the acoustical characteristics and differences between spaces only by physically moving from one place to another, for instance, from concert hall to concert hall. And aside from research projects, seldom exactly the same sounds are listened to, but commonly our perceptions and assessments of room acoustical qualities are based on the experience of different sounds in the different rooms.\nSo here we simply asked that are people able to detect and match concert halls when they are listening to same or different excitation signals?\nSo, in this experiment the listener is presented with a sound that is auralized to a particular concert hall (reference), and then she/he needs to find the same concert hall among four alternatives with different or same sound auralized in those halls. Listener is also presented with the same sounds to enable comparison of performance between the different and same sounds. The experiment was implemented in Matlab. Here is a figure of the GUI:\nThe experiment is constructed with the following variables:\n Four concert halls coded as “AC”, “BP”, “CP”, “MH”.  Each one acts as the reference in each sound case  MUSIC: Full orchestra  Two excerpts (7 s) from the same piece by Beethoven -\u0026gt; “BEE1” and “BEE2” Combinations:  same sounds: BEE1 vs BEE1; BEE2 vs BEE2 different sounds: BEE1 vs BEE2; BEE2 vs BEE1 each hall acts as a ref -\u0026gt; 4*4 = 16 trials   INSTR: Single violin  Single sounds and short passages (\u0026lt; 6 s) Combinations:  same: i.e., Violin 1 vs Violin 1 (8 trials) different: i.e., Violin 1 vs Violin 2 (8 trials) Note that these are single intrument sounds and it does not make much sense to do Violin 2 vs Violin 2, as it would be basically the same as Violin 1 vs Violin 1.   NOTE: Matching is done only within MUSIC or INSTR sounds, meaning that Beethoven is not compared to Violin sounds or vice versa.  We would have liked to include one repetition of each of the trials, but because this experiment was scheduled to be run in a single afternoon with approx. 15 people, the experiment should not take more than 30 min to complete and it was necesssary to sacrifice the repetition in order to reduce the final lenght of experiment.\nFor the uninitialized: Reading the data from .csv files in R In this experiment, I chose to set up Matlab to output the results as .csv files for each listener and then move to R for the data analysis.\nLet’s first have a peak at the data of a single individual:\n# READ SINGLE CSV FILE  So this basically illustrates also the design matrix of the experiment; the different columns hold the variables, for instance, MUS_REF indicates the sound of the REF and MUS_COM that of the comparison samples. SAMEDIFF column indicates whether the MUS_REF and MUS_COM were the same or different sounds. REF and A-D shows the hall names behind the GUI buttond and the column ANS indicates the chosen response and CORRECT indicates whether the answer was correct (1) or not (0).\n(No need to worry about some of the columns, for instance, REP does not mean anything here as there were no repetitions.)\nThen a set of individual .csv -files can be read and bind to single data.frame as follows (For illustration purposes, now here is only 4 individual .csv files):\nThe function str() summarises the variables and their types and it is always worthwhile to check that there no some funny business going on.\nNow, instead of using only 4 individuals, I have simulated random results for 20 individuals, in the hope to better reflect the real data that we are aiming at.\nThese simulated results have been saved in “SIMDATA.txt”, which is in the same format as the data.frame generated above.\nThis data set will be used in the next steps of this presentation.\n  Preliminaries Now that we have our raw dataset ready for analysis, let’s first think little about the objectives of the next steps and what we would like our data to tell us.\nSo we are interested in the following main question:\nAre listeners able to match the samples based on the room acoustics of the concert halls and are there differences in performances when they do this with the same and different excitation signals?\nThis question can then be broken down to different levels:\n Overall results (i.e., all same and all different) Results between MUSIC (Beethoven) and INSTR (Violin) Possible differences between concert halls (i.e.,are some concert halls confused between each other more than others?) Results of each individual  In the skinning of any dataset, a good first step is to try to plot the results in some way.\nHere we will simply calculate the percentages of the correct answer per each individual and then make a boxplot of the results with ggplot. (Boxplot depicts the data with the median and the interquartile range (IQR) (Q1 (25 %) - Q3 (75%)) with whiskers extending to Q1-1.5*IQR and Q3+1.5*IQR.)\nCalculate the percentages and make a corresponding data.frame:\nThen make a “long”-format table with melt()-function and plot the results with ggplot.\nNow we have the first view of our data. Note that as expected with randomised data the median percentages of correct answers set nicely on the change level of 1/4 = 25 %.\nWith another dataset one would be inclined to run a test, such as Kruskal-Wallis rank sum test (non-parametric one way anova by ranks) to test whether the distributions of the percentages of correct answers differ between the same and difference cases. Using the m1-data from above, one may run this e.g., by kruskal.test(x=m1$value, g=m1$samedifflist).\nNow, besides just looking at the correct answers we can look at the actual perceived halls (ANS column) versus the true halls (REF column).\nThis way the experiment now presents itself as a classical multiclass classification problem, where the listeners makes class “predictions” based on one’s perceptions.\nTherefore, these results are perhaps best presented and analysed in the spirit of machine learning and treated with the tools, concepts and metrics developed for the classification tasks and for the analysis of confusion matrices.\nOnto the confusion matrices Confusion matrices can be exctracted in quite a straightforward manner from our data:\n Analysis with caret-package Now that we have our set of main confusion matrices ready (we won’t be looking at individual level performance here), there a various R packages that can be used for the analysis. Here, we will be using the caret -package, and as an example, we will analyse only a single matrix.\nAnd now we are ready to run the analysis function from the caret-pckg:\n#### ANALYSIS: As shown, the confusionMatrix()-function outputs a set of performance metrics, and information on these metrics can be found from the wikipedia page. Also give a look here and here for some more tutorials on multiclass classification\nHere is a very short summary about the terminology and the derivations:\n condition positive (P) : the number of real positive cases in the data condition negative (N): the number of real negative cases in the data true positive (TP), items that are correctly classified, i.e., “hit” true negative (TN), items that are correctly classified as not belonging to the class, i.e. correct rejection false positive (FP), items that are incorrectly perceived to belong to the class, i.e., false alarm (Type I error) false negative (FN), items that are not perceived as belonging to the class but should have been; i.e., “miss” (Type II error)  The main metrics that may be interested in:\n Accuracy: The overall accuracy of the prediction (TP + TN) / P + N\n Recall; sensitivity; hit rate; true positive rate : the proportion of correct answers to total answers TP / P = TP / (TP + FN)\n Precision is the proportion of predictions that are correct from all “positive” predictions of that class TP / (TP + FP)\n  Recall and precision (and other metrics) can be calculated the following table that is generated separately for each hall :\n   Reference     Prediction Hall 1 Hall 234  Hall 1 A (TP) B (FP)  Hall 234 C (FN) D (TN)    And for your reference (from caret-pckg documentation)\n Sensitivity = A/(A+C) Specificity = D/(B+D) Prevalence = (A+C)/(A+B+C+D) Positive Predictive Value (PPV) (same as Precision) = (sensitivity * prevalence)/((sensitivity*prevalence) + ((1-specificity)*(1-prevalence))) Negative Preditive Value (NPV) = (specificity * (1-prevalence))/(((1-sensitivity)prevalence) + ((specificity)(1-prevalence))) Detection Rate = A/(A+B+C+D) Detection Prevalence = (A+B)/(A+B+C+D) Balanced Accuracy = (sensitivity+specificity)/2  The output includes Cohen’s Kappa statistic which measures the agreement between the predictions and the reference counts and it ranges from 0 to 1, so that, close to zero values indicate that the performance is not better than change level.\nIt is interesting that also P-value of McNemar’s test is included. McNemar’s test indicates whether the marginal frequecies in the confusion matrix are equal (null hypothesis) or not (alternative hypothesis). This test requires matched pairs of observations, and I do not fully understand what is the meaning here.\nHowever, McNemar’s test is actually quite useful in many cases when one can form matched pairs from the data, and for our current analysis, there is actually another use of McNemar’s test illustrated next.\nWe can generate a contingengy table between the “same” and “different” sounds from our data by forming matched pairs where the same subject has made task with a particular sound and a particular concert hall as the reference sample. The following script illustrates what I mean:\nIn this way, McNemar’s test provides as a nice direct test statistic in terms of our question whether there is a difference between between our “same” and “different” cases.\nAll together the analysis of this matching data (when it is also looked at different levels) seems now to be quite exhaustive.\n…\nFinally, in order to have some more figures to accompany the analysis here is an example of how to illustrate the results by plotting the confusion matrices with ggplot2. (in the following, xtab2 was generated the same way as xtab.)\nLeave a comment:\n   ","date":1579651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579651200,"objectID":"673b186526ed0b32da076d1937313848","permalink":"https://anttikuusinen.github.io/post/2021-04-01-onlinematching/2021-04-01-onlinematching/","publishdate":"2020-01-22T00:00:00Z","relpermalink":"/post/2021-04-01-onlinematching/2021-04-01-onlinematching/","section":"post","summary":". -- TRYING TO UPDATE previous post.\n(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)\nIntroduction The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni. over the past decade or so. There are always some ideas left in old notebooks that did not get studied at the time they emerged and this perceptual matching experiment is one of those.","tags":["R Markdown","data preprocessing","confusion matrix","classification"],"title":"Online recognition of concert hall acoustics listening experiment","type":"post"}]