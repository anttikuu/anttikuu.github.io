<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Antti Kuusinen">

  
  
  
    
  
  <meta name="description" content=". -- TRYING TO UPDATE previous post.
(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)
Introduction The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni. over the past decade or so. There are always some ideas left in old notebooks that did not get studied at the time they emerged and this perceptual matching experiment is one of those.">

  
  <link rel="alternate" hreflang="en-us" href="https://anttikuusinen.github.io/post/2021-04-01-onlinematching/2021-04-01-onlinematching/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="../../../css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-152639984-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-152639984-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="../../../index.webmanifest">
  <link rel="icon" type="image/png" href="../../../img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="../../../img/icon-192.png">

  <link rel="canonical" href="https://anttikuusinen.github.io/post/2021-04-01-onlinematching/2021-04-01-onlinematching/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Antti Kuusinen">
  <meta property="og:url" content="https://anttikuusinen.github.io/post/2021-04-01-onlinematching/2021-04-01-onlinematching/">
  <meta property="og:title" content="Online recognition of concert hall acoustics listening experiment | Antti Kuusinen">
  <meta property="og:description" content=". -- TRYING TO UPDATE previous post.
(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)
Introduction The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni. over the past decade or so. There are always some ideas left in old notebooks that did not get studied at the time they emerged and this perceptual matching experiment is one of those."><meta property="og:image" content="https://anttikuusinen.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://anttikuusinen.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-01-22T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-01-22T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://anttikuusinen.github.io/post/2021-04-01-onlinematching/2021-04-01-onlinematching/"
  },
  "headline": "Online recognition of concert hall acoustics listening experiment",
  
  "datePublished": "2020-01-22T00:00:00Z",
  "dateModified": "2020-01-22T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Antti Kuusinen"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Antti Kuusinen",
    "logo": {
      "@type": "ImageObject",
      "url": "https://anttikuusinen.github.io/img/icon-512.png"
    }
  },
  "description": ". -- TRYING TO UPDATE previous post.\n(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)\nIntroduction The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni. over the past decade or so. There are always some ideas left in old notebooks that did not get studied at the time they emerged and this perceptual matching experiment is one of those."
}
</script>

  

  


  


  





  <title>Online recognition of concert hall acoustics listening experiment | Antti Kuusinen</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="../../../">Antti Kuusinen</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="../../../#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="../../../#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="../../../files/CV_4_1_21.pdf"><span>CV (pdf)</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Online recognition of concert hall acoustics listening experiment</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jan 22, 2020
  </span>
  

  

  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="../../../categories/r/">R</a>, <a href="../../../categories/data-analysis/">Data analysis</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      


<!-- % This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. -->
<!-- When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: -->
<p>TRYING TO UPDATE
<a href="https://anttikuusinen.github.io/post/2020-22-01-chm/2020-22-01-chm/">previous post</a>.</p>
<p><em>(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)</em></p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni. over the past decade or so. There are always some ideas left in old notebooks that did not get studied at the time they emerged and this perceptual matching experiment is one of those.</p>
<p>The premise and the guiding idea in many of our previous works has been the ability to discern, evaluate and analyse even the tinyest differences and the different flavors and nuances of concert hall acoustics. Thus, previously most of our perceptual studies have been conducted so that the differences between different halls would be as easy to perceive as possible. One of the best ways to reveal small differences in room acoustics is to play back the music (signal) continuously so that when the sample is switched to another hall, only the surrounding acoustics changes while the music goes on and on. <!--(As any one who have participated in listening experiments or perform critical listening knows, also looping back a short segment of sound and quickly switching back and forth between different samples is often a very revealing strategy to pinpoint small differences and nuances in sounds.)--></p>
<p>However, the acoustic “teleportation” is only possible at the lab, and in natural environments we may perceptually evaluate the acoustical characteristics and differences between spaces only by physically moving from one place to another, for instance, from concert hall to concert hall. And aside from research projects, seldom exactly the same sounds are listened to, but commonly our perceptions and assessments of room acoustical qualities are based on the experience of different sounds in the different rooms.</p>
<p>So here we simply asked that <em>are people able to detect and match concert halls when they are listening to same or different excitation signals?</em></p>
<p>So, in this experiment the listener is presented with a sound that is auralized to a particular concert hall (reference), and then she/he needs to find the same concert hall among four alternatives with different or same sound auralized in those halls. Listener is also presented with the same sounds to enable comparison of performance between the different and same sounds. The experiment was implemented in Matlab. Here is a figure of the GUI:</p>
<p>The experiment is constructed with the following variables:</p>
<ul>
<li>Four concert halls coded as “AC”, “BP”, “CP”, “MH”.
<ul>
<li>Each one acts as the reference in each sound case</li>
</ul></li>
<li>MUSIC: Full orchestra
<ul>
<li>Two excerpts (7 s) from the same piece by Beethoven -&gt; “BEE1” and “BEE2”</li>
<li>Combinations:
<ul>
<li>same sounds: BEE1 vs BEE1; BEE2 vs BEE2</li>
<li>different sounds: BEE1 vs BEE2; BEE2 vs BEE1</li>
<li>each hall acts as a ref -&gt; 4*4 = 16 trials</li>
</ul></li>
</ul></li>
<li>INSTR: Single violin
<ul>
<li>Single sounds and short passages (&lt; 6 s)</li>
<li>Combinations:
<ul>
<li>same: i.e., Violin 1 vs Violin 1 (8 trials)</li>
<li>different: i.e., Violin 1 vs Violin 2 (8 trials)</li>
<li>Note that these are single intrument sounds and it does not make much sense to do Violin 2 vs Violin 2, as it would be basically the same as Violin 1 vs Violin 1.</li>
</ul></li>
</ul></li>
<li>NOTE: Matching is done only within MUSIC or INSTR sounds, meaning that Beethoven is not compared to Violin sounds or vice versa.</li>
</ul>
<p>We would have liked to include one repetition of each of the trials, but because this experiment was scheduled to be run in a single afternoon with approx. 15 people, the experiment should not take more than 30 min to complete and it was necesssary to sacrifice the repetition in order to reduce the final lenght of experiment.</p>
<div id="for-the-uninitialized-reading-the-data-from-.csv-files-in-r" class="section level3">
<h3>For the uninitialized: Reading the data from .csv files in R</h3>
<p>In this experiment, I chose to set up Matlab to output the results as .csv files for each listener and then move to R for the data analysis.</p>
<p>Let’s first have a peak at the data of a single individual:</p>
<pre class="r"><code>#  READ SINGLE CSV FILE </code></pre>
<p>So this basically illustrates also the design matrix of the experiment; the different columns hold the variables, for instance, MUS_REF indicates the sound of the REF and MUS_COM that of the comparison samples. SAMEDIFF column indicates whether the MUS_REF and MUS_COM were the same or different sounds. REF and A-D shows the hall names behind the GUI buttond and the column ANS indicates the chosen response and CORRECT indicates whether the answer was correct (1) or not (0).<br />
(No need to worry about some of the columns, for instance, REP does not mean anything here as there were no repetitions.)</p>
<p>Then a set of individual .csv -files can be read and bind to single data.frame as follows (For illustration purposes, now here is only 4 individual .csv files):</p>
<p>The function <strong>str()</strong> summarises the variables and their types and it is always worthwhile to check that there no some funny business going on.</p>
<p>Now, instead of using only 4 individuals, I have simulated random results for 20 individuals, in the hope to better reflect the real data that we are aiming at.</p>
<p>These simulated results have been saved in “SIMDATA.txt”, which is in the same format as the data.frame generated above.</p>
<p>This data set will be used in the next steps of this presentation.</p>
</div>
</div>
<div id="preliminaries" class="section level2">
<h2>Preliminaries</h2>
<p>Now that we have our <strong>raw</strong> dataset ready for analysis, let’s first think little about the objectives of the next steps and what we would like our data to tell us.</p>
<p>So we are interested in the following main question:</p>
<p><strong>Are listeners able to match the samples based on the room acoustics of the concert halls and are there differences in performances when they do this with the same and different excitation signals?</strong></p>
<p>This question can then be broken down to different levels:</p>
<ul>
<li>Overall results (i.e., all same and all different)</li>
<li>Results between MUSIC (Beethoven) and INSTR (Violin)</li>
<li>Possible differences between concert halls (i.e.,are some concert halls confused between each other more than others?)</li>
<li>Results of each individual</li>
</ul>
<p>In the skinning of any dataset, a good first step is to try to plot the results in some way.</p>
<p>Here we will simply calculate the percentages of the correct answer per each individual and then make a boxplot of the results with ggplot. (Boxplot depicts the data with the median and the interquartile range (IQR) (Q1 (25 %) - Q3 (75%)) with whiskers extending to Q1-1.5*IQR and Q3+1.5*IQR.)</p>
<p>Calculate the percentages and make a corresponding data.frame:</p>
<p>Then make a “long”-format table with melt()-function and plot the results with ggplot.</p>
<p>Now we have the first view of our data. Note that as expected with randomised data the median percentages of correct answers set nicely on the change level of 1/4 = 25 %.</p>
<p>With another dataset one would be inclined to run a test, such as Kruskal-Wallis rank sum test (non-parametric one way anova by ranks) to test whether the distributions of the percentages of correct answers differ between the same and difference cases. Using the m1-data from above, one may run this e.g., by kruskal.test(x=m1$value, g=m1$samedifflist).</p>
<p>Now, besides just looking at the correct answers we can look at the actual perceived halls (ANS column) versus the true halls (REF column).</p>
<p>This way the experiment now presents itself as a classical multiclass classification problem, where the listeners makes class “predictions” based on one’s perceptions.</p>
<p>Therefore, these results are perhaps best presented and analysed in the spirit of machine learning and treated with the tools, concepts and metrics developed for the classification tasks and for the analysis of confusion matrices.</p>
<div id="onto-the-confusion-matrices" class="section level3">
<h3>Onto the confusion matrices</h3>
<p>Confusion matrices can be exctracted in quite a straightforward manner from our data:</p>
</div>
<div id="analysis-with-caret-package" class="section level3">
<h3>Analysis with caret-package</h3>
<p>Now that we have our set of main confusion matrices ready (we won’t be looking at individual level performance here), there a various R packages that can be used for the analysis. Here, we will be using the caret -package, and as an example, we will analyse only a single matrix.</p>
<p>And now we are ready to run the analysis function from the caret-pckg:</p>
<pre class="r"><code>#### ANALYSIS:</code></pre>
<p>As shown, the confusionMatrix()-function outputs a set of performance metrics, and information on these metrics can be found from the <a href="https://en.wikipedia.org/wiki/Confusion_matrix">wikipedia page</a>. Also give a look <a href="https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2">here</a> and <a href="https://www.datascienceblog.net/post/machine-learning/performance-measures-multi-class-problems/">here</a> for some more tutorials on multiclass classification</p>
<p>Here is a very short summary about the terminology and the derivations:</p>
<ul>
<li>condition positive (<strong>P</strong>) : the number of real positive cases in the data</li>
<li>condition negative (<strong>N</strong>): the number of real negative cases in the data</li>
<li>true positive (<strong>TP</strong>), items that are correctly classified, i.e., “hit”</li>
<li>true negative (<strong>TN</strong>), items that are correctly classified as not belonging to the class, i.e. correct rejection</li>
<li>false positive (<strong>FP</strong>), items that are incorrectly perceived to belong to the class, i.e., false alarm (Type I error)</li>
<li>false negative (<strong>FN</strong>), items that are not perceived as belonging to the class but should have been; i.e., “miss” (Type II error)</li>
</ul>
<p>The main metrics that may be interested in:</p>
<ul>
<li><p><strong>Accuracy</strong>: The overall accuracy of the prediction (TP + TN) / P + N</p></li>
<li><p><strong>Recall</strong>; <strong>sensitivity</strong>; <strong>hit rate</strong>; <strong>true positive rate</strong> : the proportion of correct answers to total answers TP / P = TP / (TP + FN)</p></li>
<li><p><strong>Precision</strong> is the proportion of predictions that are correct from all “positive” predictions of that class TP / (TP + FP)</p></li>
</ul>
<p>Recall and precision (and other metrics) can be calculated the following table that is generated separately for each hall :</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Reference</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Prediction</strong></td>
<td>Hall 1</td>
<td>Hall 234</td>
</tr>
<tr class="even">
<td>Hall 1</td>
<td>A (TP)</td>
<td>B (FP)</td>
</tr>
<tr class="odd">
<td>Hall 234</td>
<td>C (FN)</td>
<td>D (TN)</td>
</tr>
</tbody>
</table>
<p>And for your reference (from caret-pckg documentation)</p>
<ul>
<li>Sensitivity = A/(A+C)</li>
<li>Specificity = D/(B+D)</li>
<li>Prevalence = (A+C)/(A+B+C+D)</li>
<li>Positive Predictive Value (PPV) (same as Precision) = (sensitivity * prevalence)/((sensitivity*prevalence) + ((1-specificity)*(1-prevalence)))</li>
<li>Negative Preditive Value (NPV) = (specificity * (1-prevalence))/(((1-sensitivity)<em>prevalence) +
((specificity)</em>(1-prevalence)))</li>
<li>Detection Rate = A/(A+B+C+D)</li>
<li>Detection Prevalence = (A+B)/(A+B+C+D)</li>
<li>Balanced Accuracy = (sensitivity+specificity)/2</li>
</ul>
<p>The output includes <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen’s <strong>Kappa</strong> statistic</a> which measures the agreement between the predictions and the reference counts and it ranges from 0 to 1, so that, close to zero values indicate that the performance is not better than change level.</p>
<p>It is interesting that also P-value of <a href="https://en.wikipedia.org/wiki/McNemar%27s_test">McNemar’s test</a> is included. McNemar’s test indicates whether the marginal frequecies in the confusion matrix are equal (null hypothesis) or not (alternative hypothesis). This test requires matched pairs of observations, and I do not fully understand what is the meaning here.</p>
<p>However, McNemar’s test is actually quite useful in many cases when one can form matched pairs from the data, and for our current analysis, there is actually another use of McNemar’s test illustrated next.</p>
<p>We can generate a contingengy table between the “same” and “different” sounds from our data by forming matched pairs where the same subject has made task with a particular sound and a particular concert hall as the reference sample. The following script illustrates what I mean:</p>
<p>In this way, McNemar’s test provides as a nice direct test statistic in terms of our question whether there is a difference between between our “same” and “different” cases.</p>
<p>All together the analysis of this matching data (when it is also looked at different levels) seems now to be quite exhaustive.</p>
<p>…</p>
<p>Finally, in order to have some more figures to accompany the analysis here is an example of how to illustrate the results by plotting the confusion matrices with ggplot2. (in the following, xtab2 was generated the same way as xtab.)</p>
<hr />
<p>Leave a comment:</p>
<script src="https://utteranc.es/client.js"
        repo="anttikuusinen/anttikuusinen.github.io"
        issue-term="[CHMdata]"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>
</div>
</div>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="../../../tags/r-markdown/">R Markdown</a>
  
  <a class="badge badge-light" href="../../../tags/data-preprocessing/">data preprocessing</a>
  
  <a class="badge badge-light" href="../../../tags/confusion-matrix/">confusion matrix</a>
  
  <a class="badge badge-light" href="../../../tags/classification/">classification</a>
  
</div>














  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="../../../authors/admin/avatar_hu5eefcd357d44e1910a828a3ff6ecb76d_370497_250x250_fill_q90_lanczos_center.JPG" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://anttikuusinen.github.io/">Antti Kuusinen</a></h5>
      <h6 class="card-subtitle">Postdoctoral Researcher</h6>
      <p class="card-text">My research interests include the human perception of speech, music and noise in reverberant spaces.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:antti.kuusinen@aalto.fi" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/anttikuusinen/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.fi/citations?user=HJkdqi8AAAAJ&amp;hl=fi" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Antti_Kuusinen" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/anttikuusinen/" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/0000-0002-8106-0421" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="../../../post/2020-22-01-chm/2020-22-01-chm/">On the data analysis of perceptual matching of concert hall acoustics</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

      
      
    

    
    
      <script async defer src="https://maps.googleapis.com/maps/api/js?key="></script>
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/gmaps.js/0.4.25/gmaps.min.js" integrity="sha256-7vjlAeb8OaTrCXZkCNun9djzuB2owUsaO72kXaFDBJs=" crossorigin="anonymous"></script>
      
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="../../../js/academic.min.d6bd04fdad2ad213aa8111c5a3b72fc5.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    ©2021 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
