<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Antti Kuusinen">

  
  
  
    
  
  <meta name="description" content=". -- In this post I will briefly discuss and illustrate the analysis of perceptual matching experiment on concert hall acoustics. How the sound samples / auralizations of concert hall acoustics in this experiment were made is a topic of another time.
(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)
Introduction The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni.">

  
  <link rel="alternate" hreflang="en-us" href="https://anttikuusinen.github.io/post/2020-22-01-chm/2020-22-01-chm/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="../../../css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-152639984-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'UA-152639984-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="../../../index.webmanifest">
  <link rel="icon" type="image/png" href="../../../img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="../../../img/icon-192.png">

  <link rel="canonical" href="https://anttikuusinen.github.io/post/2020-22-01-chm/2020-22-01-chm/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Antti Kuusinen">
  <meta property="og:url" content="https://anttikuusinen.github.io/post/2020-22-01-chm/2020-22-01-chm/">
  <meta property="og:title" content="On the data analysis of perceptual matching of concert hall acoustics | Antti Kuusinen">
  <meta property="og:description" content=". -- In this post I will briefly discuss and illustrate the analysis of perceptual matching experiment on concert hall acoustics. How the sound samples / auralizations of concert hall acoustics in this experiment were made is a topic of another time.
(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)
Introduction The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni."><meta property="og:image" content="https://anttikuusinen.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://anttikuusinen.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-01-22T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-01-22T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://anttikuusinen.github.io/post/2020-22-01-chm/2020-22-01-chm/"
  },
  "headline": "On the data analysis of perceptual matching of concert hall acoustics",
  
  "datePublished": "2020-01-22T00:00:00Z",
  "dateModified": "2020-01-22T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Antti Kuusinen"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Antti Kuusinen",
    "logo": {
      "@type": "ImageObject",
      "url": "https://anttikuusinen.github.io/img/icon-512.png"
    }
  },
  "description": ". -- In this post I will briefly discuss and illustrate the analysis of perceptual matching experiment on concert hall acoustics. How the sound samples / auralizations of concert hall acoustics in this experiment were made is a topic of another time.\n(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)\nIntroduction The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni."
}
</script>

  

  


  


  





  <title>On the data analysis of perceptual matching of concert hall acoustics | Antti Kuusinen</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="../../../">Antti Kuusinen</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="../../../#about"><span>About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="../../../#datasets"><span>Datasets</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="../../../#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="../../../files/CV_4_1_21.pdf"><span>CV (pdf)</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>On the data analysis of perceptual matching of concert hall acoustics</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jan 22, 2020
  </span>
  

  

  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="../../../categories/r/">R</a>, <a href="../../../categories/data-analysis/">Data analysis</a></span>
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      


<!-- % This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. -->
<!-- When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: -->
<p>In this post I will briefly discuss and illustrate the analysis of perceptual matching experiment on concert hall acoustics. How the sound samples / auralizations of concert hall acoustics in this experiment were made is a topic of another time.</p>
<p><em>(This presentation is prepared for and presented in the “Acoutect Research and Demonstrator Workshop 5” held at Aalto University in 20-24 January 2020)</em></p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The background of the experiment is on our previous research on concert hall acoustics with the loudspeaker orchestra and spatial sound / multichannel auralization techniques that we have conducted at Aalto Uni. over the past decade or so. There are always some ideas left in old notebooks that did not get studied at the time they emerged and this perceptual matching experiment is one of those.</p>
<p>The premise and the guiding idea in many of our previous works has been the ability to discern, evaluate and analyse even the tinyest differences and the different flavors and nuances of concert hall acoustics. Thus, previously most of our perceptual studies have been conducted so that the differences between different halls would be as easy to perceive as possible. One of the best ways to reveal small differences in room acoustics is to play back the music (signal) continuously so that when the sample is switched to another hall, only the surrounding acoustics changes while the music goes on and on. <!--(As any one who have participated in listening experiments or perform critical listening knows, also looping back a short segment of sound and quickly switching back and forth between different samples is often a very revealing strategy to pinpoint small differences and nuances in sounds.)--></p>
<p>However, the acoustic “teleportation” is only possible at the lab, and in natural environments we may perceptually evaluate the acoustical characteristics and differences between spaces only by physically moving from one place to another, for instance, from concert hall to concert hall. And aside from research projects, seldom exactly the same sounds are listened to, but commonly our perceptions and assessments of room acoustical qualities are based on the experience of different sounds in the different rooms.</p>
<p>So here we simply asked that <em>are people able to detect and match concert halls when they are listening to same or different excitation signals?</em></p>
<p>So, in this experiment the listener is presented with a sound that is auralized to a particular concert hall (reference), and then she/he needs to find the same concert hall among four alternatives with different or same sound auralized in those halls. Listener is also presented with the same sounds to enable comparison of performance between the different and same sounds. The experiment was implemented in Matlab. Here is a figure of the GUI:</p>
<p><img src="../../../img/match.png" style="width:50.0%" /></p>
<p>The experiment is constructed with the following variables:</p>
<ul>
<li>Four concert halls coded as “AC”, “BP”, “CP”, “MH”.
<ul>
<li>Each one acts as the reference in each sound case</li>
</ul></li>
<li>MUSIC: Full orchestra
<ul>
<li>Two excerpts (7 s) from the same piece by Beethoven -&gt; “BEE1” and “BEE2”</li>
<li>Combinations:
<ul>
<li>same sounds: BEE1 vs BEE1; BEE2 vs BEE2</li>
<li>different sounds: BEE1 vs BEE2; BEE2 vs BEE1</li>
<li>each hall acts as a ref -&gt; 4*4 = 16 trials</li>
</ul></li>
</ul></li>
<li>INSTR: Single violin
<ul>
<li>Single sounds and short passages (&lt; 6 s)</li>
<li>Combinations:
<ul>
<li>same: i.e., Violin 1 vs Violin 1 (8 trials)</li>
<li>different: i.e., Violin 1 vs Violin 2 (8 trials)</li>
<li>Note that these are single intrument sounds and it does not make much sense to do Violin 2 vs Violin 2, as it would be basically the same as Violin 1 vs Violin 1.</li>
</ul></li>
</ul></li>
<li>NOTE: Matching is done only within MUSIC or INSTR sounds, meaning that Beethoven is not compared to Violin sounds or vice versa.</li>
</ul>
<p>We would have liked to include one repetition of each of the trials, but because this experiment was scheduled to be run in a single afternoon with approx. 15 people, the experiment should not take more than 30 min to complete and it was necesssary to sacrifice the repetition in order to reduce the final lenght of experiment.</p>
<div id="for-the-uninitialized-reading-the-data-from-.csv-files-in-r" class="section level3">
<h3>For the uninitialized: Reading the data from .csv files in R</h3>
<p>In this experiment, I chose to set up Matlab to output the results as .csv files for each listener and then move to R for the data analysis.</p>
<p>Let’s first have a peak at the data of a single individual:</p>
<pre class="r"><code>#  READ SINGLE CSV FILE 
test &lt;- read.csv2(&#39;S01.csv&#39;, sep = &quot;,&quot;)
head(test)</code></pre>
<pre><code>##   SUB_ID ORD CASE_ID SOUNDTYPE SOUNDNUM SAMEDIFF MUS_REF MUS_COM REF  A  B  C
## 1    S01   1       1     MUSIC        0     same    BEE1    BEE1  AC CP AC MH
## 2    S01   2       2     MUSIC        0     same    BEE1    BEE1  BP BP AC CP
## 3    S01   3       5     MUSIC        0     diff    BEE1    BEE2  AC BP AC MH
## 4    S01   4      23     INSTR        7     diff  viulut viulut2  MH AC BP CP
## 5    S01   5       3     MUSIC        0     same    BEE1    BEE1  MH MH BP CP
## 6    S01   6      14     MUSIC        0     same    BEE2    BEE2  BP AC BP CP
##    D  ANS CORRECT                 TIME COMPLETED REP
## 1 BP   BP       0 14-Jan-2020 13:34:28         1   1
## 2 MH &lt;NA&gt;       0 13-Jan-2020 15:27:52         0   1
## 3 CP &lt;NA&gt;       0 13-Jan-2020 15:27:52         0   1
## 4 MH &lt;NA&gt;       0 13-Jan-2020 15:27:52         0   1
## 5 AC &lt;NA&gt;       0 13-Jan-2020 15:27:52         0   1
## 6 MH &lt;NA&gt;       0 13-Jan-2020 15:27:52         0   1</code></pre>
<p>So this basically illustrates also the design matrix of the experiment; the different columns hold the variables, for instance, MUS_REF indicates the sound of the REF and MUS_COM that of the comparison samples. SAMEDIFF column indicates whether the MUS_REF and MUS_COM were the same or different sounds. REF and A-D shows the hall names behind the GUI buttond and the column ANS indicates the chosen response and CORRECT indicates whether the answer was correct (1) or not (0).<br />
(No need to worry about some of the columns, for instance, REP does not mean anything here as there were no repetitions.)</p>
<p>Then a set of individual .csv -files can be read and bind to single data.frame as follows (For illustration purposes, now here is only 4 individual .csv files):</p>
<pre class="r"><code># LIST .csv files
csvfiles = list.files(pattern = &#39;.csv&#39;)
# Make an empty list
chm_data &lt;- list();
# Populate the list entries from the .csv-files
for (rfile in 1:length(csvfiles)){
    filename &lt;- csvfiles[rfile]
    chm_data[[rfile]] &lt;- read.csv2(filename, sep = &quot;,&quot;)
} 
# ROW BIND THE LIST ELEMENTS: &quot;UNLIST&quot; (the unlist() -function outputs an atomic vector of all elements and does not work here)
chm_df &lt;- do.call(rbind, chm_data) # 
# CAST AS DATA.FRAME:
chm_df &lt;- data.frame(chm_df)

# not run here
#summary(chm_df)
# SAVE AS TXT:
#write.table(chm_df, file = &#39;LONGTABLE2.txt&#39;, sep = &#39;,&#39;, quote = F)
# OR AS .RData:
#save(chm_df, file = &quot;LONGTABLE2.RData&quot;)

# CHECK data
str(chm_df)</code></pre>
<pre><code>## &#39;data.frame&#39;:    192 obs. of  18 variables:
##  $ SUB_ID   : chr  &quot;S01&quot; &quot;S01&quot; &quot;S01&quot; &quot;S01&quot; ...
##  $ ORD      : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ CASE_ID  : int  1 2 5 23 3 14 4 24 10 8 ...
##  $ SOUNDTYPE: chr  &quot;MUSIC&quot; &quot;MUSIC&quot; &quot;MUSIC&quot; &quot;INSTR&quot; ...
##  $ SOUNDNUM : int  0 0 0 7 0 0 0 8 0 0 ...
##  $ SAMEDIFF : chr  &quot;same&quot; &quot;same&quot; &quot;diff&quot; &quot;diff&quot; ...
##  $ MUS_REF  : chr  &quot;BEE1&quot; &quot;BEE1&quot; &quot;BEE1&quot; &quot;viulut&quot; ...
##  $ MUS_COM  : chr  &quot;BEE1&quot; &quot;BEE1&quot; &quot;BEE2&quot; &quot;viulut2&quot; ...
##  $ REF      : chr  &quot;AC&quot; &quot;BP&quot; &quot;AC&quot; &quot;MH&quot; ...
##  $ A        : chr  &quot;CP&quot; &quot;BP&quot; &quot;BP&quot; &quot;AC&quot; ...
##  $ B        : chr  &quot;AC&quot; &quot;AC&quot; &quot;AC&quot; &quot;BP&quot; ...
##  $ C        : chr  &quot;MH&quot; &quot;CP&quot; &quot;MH&quot; &quot;CP&quot; ...
##  $ D        : chr  &quot;BP&quot; &quot;MH&quot; &quot;CP&quot; &quot;MH&quot; ...
##  $ ANS      : chr  &quot;BP&quot; NA NA NA ...
##  $ CORRECT  : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ TIME     : chr  &quot;14-Jan-2020 13:34:28&quot; &quot;13-Jan-2020 15:27:52&quot; &quot;13-Jan-2020 15:27:52&quot; &quot;13-Jan-2020 15:27:52&quot; ...
##  $ COMPLETED: int  1 0 0 0 0 0 0 0 0 0 ...
##  $ REP      : int  1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<p>The function <strong>str()</strong> summarises the variables and their types and it is always worthwhile to check that there no some funny business going on.</p>
<p>Now, instead of using only 4 individuals, I have simulated random results for 20 individuals, in the hope to better reflect the real data that we are aiming at.</p>
<p>These simulated results have been saved in “SIMDATA.txt”, which is in the same format as the data.frame generated above.</p>
<p>This data set will be used in the next steps of this presentation.</p>
<pre class="r"><code># First remove variables from workspace
rm(list = ls()) 
# and read the data.table:
simdata &lt;- read.table(&quot;SIMDATA.txt&quot;, sep = &quot;,&quot;, header = T,stringsAsFactors = T)
 head(simdata)</code></pre>
<pre><code>##   SUB_ID ORD CASE_ID SOUNDTYPE SOUNDNUM SAMEDIFF MUS_REF MUS_COM REF  A  B  C
## 1     S1   1      21     INSTR        5     diff  viulut viulut2  AC AC BP MH
## 2     S1   2      31     INSTR       15     diff  viulut viulut2  MH AC MH CP
## 3     S1   3       4     MUSIC        0     same    BEE1    BEE1  CP MH AC BP
## 4     S1   4       3     MUSIC        0     same    BEE1    BEE1  MH MH AC BP
## 5     S1   5       7     MUSIC        0     diff    BEE1    BEE2  MH AC MH BP
## 6     S1   6      13     MUSIC        0     same    BEE2    BEE2  AC CP AC MH
##    D ANS CORRECT                 TIME COMPLETED REP
## 1 CP  MH       0 18-Jan-2020 21:09:13         0   1
## 2 BP  CP       0 18-Jan-2020 21:09:13         0   1
## 3 CP  MH       0 18-Jan-2020 21:09:12         0   0
## 4 CP  AC       0 18-Jan-2020 21:09:12         0   0
## 5 CP  BP       0 18-Jan-2020 21:09:12         0   0
## 6 BP  CP       0 18-Jan-2020 21:09:12         0   1</code></pre>
</div>
</div>
<div id="preliminaries" class="section level2">
<h2>Preliminaries</h2>
<p>Now that we have our <strong>raw</strong> dataset ready for analysis, let’s first think little about the objectives of the next steps and what we would like our data to tell us.</p>
<p>So we are interested in the following main question:</p>
<p><strong>Are listeners able to match the samples based on the room acoustics of the concert halls and are there differences in performances when they do this with the same and different excitation signals?</strong></p>
<p>This question can then be broken down to different levels:</p>
<ul>
<li>Overall results (i.e., all same and all different)</li>
<li>Results between MUSIC (Beethoven) and INSTR (Violin)</li>
<li>Possible differences between concert halls (i.e.,are some concert halls confused between each other more than others?)</li>
<li>Results of each individual</li>
</ul>
<p>In the skinning of any dataset, a good first step is to try to plot the results in some way.</p>
<p>Here we will simply calculate the percentages of the correct answer per each individual and then make a boxplot of the results with ggplot. (Boxplot depicts the data with the median and the interquartile range (IQR) (Q1 (25 %) - Q3 (75%)) with whiskers extending to Q1-1.5*IQR and Q3+1.5*IQR. CAUTIONARY NOTE: Boxplot is actually not the best way to illustrate this type of data, and it is not used in the article.)</p>
<p>Calculate the percentages and make a corresponding data.frame:</p>
<pre class="r"><code>SUBS &lt;- levels(simdata$SUB_ID)
dataf &lt;- c()
ids &lt;- c()
for(s in levels(simdata$SUB_ID)) {
    sdata &lt;- subset(simdata, SUB_ID == s)   # EXTRACT SUBJECT DATA
    ids &lt;- c(ids, s) # KEEP TRACK OF SUB ID
    musvec &lt;- c()   # init
    sdtemp &lt;- c()   # init
    for(ss in levels(sdata$SAMEDIFF)) {         
            samediffdata &lt;- subset(sdata, SAMEDIFF == ss)   # EXTRACT same or diff DATA 
            # GET THE PERCENTAGE OF CORRECT ANS
            sdtemp[ss] &lt;- as.numeric(round(sum(samediffdata$CORRECT)/length(samediffdata$CORRECT), digits = 2))         
            
            mustemp &lt;- c()  # init  
            for(sss in levels(samediffdata$MUS_REF)) {
                musdata &lt;- subset(samediffdata, MUS_REF == sss) # ECTRACT DATA by sound
                mustemp[sss] &lt;- round(sum(musdata$CORRECT)/length(musdata$CORRECT), digits = 2) # GET THE PERCENTAGE OF CORRECT ANS
                names(mustemp)[names(mustemp) == sss] &lt;-  c(paste(musdata$MUS_REF[[1]],&#39;-&#39;,musdata$MUS_COM[[1]], sep = &quot;&quot;)) # KEEP TRACK OF THE COMPARISONS
        }       
        musvec &lt;- c(musvec, mustemp) # POPULATE
    }   
    sdvec &lt;- c(sdtemp, musvec) # POPULATE
    dataf &lt;- rbind(dataf, sdvec) # POPULATE
}
# SOME CLEANING UP AND FINALISING OF THE DATAFRAME:
SAMEDIFF &lt;- data.frame(matrix(as.numeric(dataf[,1:8]),nrow = length(SUBS))) 
colnames(SAMEDIFF) &lt;- c(names(sdtemp),names(musvec))
SAMEDIFF &lt;- cbind(ids, SAMEDIFF)
colnames(SAMEDIFF)[1] &lt;- &#39;ID&#39; 
head(SAMEDIFF) # show first 6 rows</code></pre>
<pre><code>##    ID diff same BEE1-BEE2 BEE2-BEE1 viulut-viulut2 BEE1-BEE1 BEE2-BEE2
## 1  S1 0.19 0.19      0.50      0.25           0.00      0.25      0.00
## 2 S10 0.25 0.25      0.50      0.25           0.12      0.00      0.25
## 3 S11 0.31 0.38      0.00      0.25           0.50      0.50      0.25
## 4 S12 0.19 0.19      0.00      0.50           0.12      0.00      0.00
## 5 S13 0.44 0.25      0.75      0.25           0.38      0.25      0.50
## 6 S14 0.31 0.25      0.00      0.50           0.38      0.00      0.00
##   viulut-viulut
## 1          0.25
## 2          0.38
## 3          0.38
## 4          0.38
## 5          0.12
## 6          0.50</code></pre>
<p>Then make a “long”-format table with melt()-function and plot the results with ggplot.</p>
<pre class="r"><code>#### BOXPLOTS: OVERALL SAME vs DIFF
library(reshape2) # melt()-function
# melt the data into &quot;long&quot;-format for ggplotting
m0 = melt(SAMEDIFF[,c(&#39;same&#39;,&#39;diff&#39;)],measure.vars = c(&#39;same&#39;, &#39;diff&#39;), variable.name = &#39;G&#39;)

library(ggplot2)

g1 &lt;- ggplot(data = m0, aes(y = value, x = G)) +  theme_bw() + 
  geom_boxplot(outlier.shape = NA) +
geom_point(aes(y = value, x = G, shape = G), position = position_jitter(width = 0.2), size = 1, alpha = 1, show.legend = F, color = &#39;grey&#39;) +
coord_cartesian(ylim = c(0,1)) + labs(title =  &#39;&#39;, x = &#39;&#39;, y = &#39;Percentage of correct answers&#39;, size = 0.1, color = &#39;black&#39;) + 
theme(axis.title.y = element_text(size = 8)) + ggtitle(&#39;Overall&#39;)

# FOR SAVING THE PLOT: (not executed)
#ggsave(filename = &#39;SRT_basic_boxplot.eps&#39;, g1, width = 10, height = 6, units = &#39;cm&#39;)

#### BOXPLOTS: MUSIC (Beethoven) vs INSTR (Violin)
m1 = melt(SAMEDIFF[,colnames(SAMEDIFF)[4:9]], measure.vars = colnames(SAMEDIFF)[4:9], variable.name = &#39;musiclist&#39;)

samedifflist &lt;- c(rep(&#39;diff&#39;, 60),rep(&#39;same&#39;, 60))
muslist &lt;- c(rep(&#39;Beethoven&#39;, 40), rep(&#39;Violin&#39;, 20), rep(&#39;Beethoven&#39;, 40), rep(&#39;Violin&#39;, 20))

m1 &lt;- cbind(m1,samedifflist, muslist)

g2 &lt;- ggplot(data = m1, aes(y = value, x = samedifflist,colour = muslist),show.legend = F) + 
theme_bw() + 
geom_boxplot(outlier.shape = NA,show.legend = F)+
geom_point(aes(y = value, colour = muslist), position = position_jitter(width = 0.2), size = 0.5, alpha = 0.5, show.legend = F, color = &#39;grey&#39;) +
facet_grid(.~muslist) +
coord_cartesian(ylim = c(0,1)) + labs(title =  &#39;&#39;, x = &#39;&#39;, y = &#39;Percentage of correct answers&#39;, size = 0.1, color = &#39;black&#39;) + 
theme(axis.title.y = element_text(size = 8)) + ggtitle(&#39;Per music&#39;)

library(gridExtra) # for grid.arrange()
grid.arrange(g1,g2,nrow = 1)</code></pre>
<p><img src="../../../post/2020-22-01-chm/2020-22-01-chm_files/figure-html/boxplots-1.png" width="576" /></p>
<p>Now we have the first view of our data. Note that as expected with randomised data the median percentages of correct answers set nicely on the change level of 1/4 = 25 %.</p>
<p>With another dataset one would be inclined to run a test, such as Kruskal-Wallis rank sum test (non-parametric one way anova by ranks) to test whether the distributions of the percentages of correct answers differ between the same and difference cases. Using the m1-data from above, one may run this e.g., by kruskal.test(x=m1$value, g=m1$samedifflist).</p>
<p>Now, besides just looking at the correct answers we can look at the actual perceived halls (ANS column) versus the true halls (REF column).</p>
<p>This way the experiment now presents itself as a classical multiclass classification problem, where the listeners makes class “predictions” based on one’s perceptions.</p>
<p>Therefore, these results are perhaps best presented and analysed in the spirit of machine learning and treated with the tools, concepts and metrics developed for the classification tasks and for the analysis of confusion matrices.</p>
<div id="onto-the-confusion-matrices" class="section level3">
<h3>Onto the confusion matrices</h3>
<p>Confusion matrices can be exctracted in quite a straightforward manner from our data:</p>
<pre class="r"><code>halls &lt;- levels(simdata$REF)

# OVERALL SAME:
hallmat1 &lt;- matrix(0,nrow = 4, ncol = 4);
rownames(hallmat1) &lt;- halls
colnames(hallmat1) &lt;- halls
samedata &lt;- subset(simdata, SAMEDIFF == &#39;same&#39;)
for(i in 1:nrow(samedata)){
    hallmat1[as.character(samedata$REF[i]), as.character(samedata$ANS[i])] &lt;- hallmat1[as.character(samedata$REF[i]), as.character(samedata$ANS[i])] + 1
}

# OVERALL DIFF: 
hallmat2 &lt;- matrix(0,nrow = 4, ncol = 4);
rownames(hallmat2) &lt;- halls
colnames(hallmat2) &lt;- halls
diffdata &lt;- subset(simdata, SAMEDIFF == &#39;diff&#39;)

for(i in 1:nrow(samedata)){
    hallmat2[as.character(diffdata$REF[i]),as.character(diffdata$ANS[i])] &lt;- hallmat2[as.character(diffdata$REF[i]),as.character(diffdata$ANS[i])] + 1
}

# PER MUSIC, SAME
samemats &lt;- list()
for(s in levels(simdata$MUS_REF)) {
hallmat &lt;- matrix(0,nrow = 4, ncol = 4);
rownames(hallmat) &lt;- halls
colnames(hallmat) &lt;- halls
data &lt;- subset(simdata, SAMEDIFF == &#39;same&#39; &amp; MUS_REF == as.character(s))
    for(i in 1:nrow(data)){
    hallmat[as.character(data$REF[i]),as.character(data$ANS[i])] &lt;- hallmat[as.character(data$REF[i]), as.character(data$ANS[i])] + 1
    }
samemats[[as.character(s)]] &lt;- hallmat
}

# PER MUSIC, DIFF
diffmats &lt;- list()
for(s in levels(simdata$MUS_REF)) {
hallmat &lt;- matrix(0,nrow = 4, ncol = 4);
rownames(hallmat) &lt;- halls
colnames(hallmat) &lt;- halls
data &lt;- subset(simdata, SAMEDIFF == &#39;diff&#39; &amp; MUS_REF == as.character(s))
    for(i in 1:nrow(data)){
    hallmat[as.character(data$REF[i]),as.character(data$ANS[i])] &lt;- hallmat[as.character(data$REF[i]), as.character(data$ANS[i])] + 1
    }
diffmats[[as.character(s)]] &lt;- hallmat
}


# PER MUSIC, MUSIC VS MUSIC:
musmatsS &lt;- list()
for(s in levels(simdata$MUS_REF)) {
    musmatsSS &lt;- list()
    data &lt;- subset(simdata, MUS_REF == as.character(s))
    for(ss in levels(simdata$MUS_COM)) {
        data2 &lt;- subset(data, MUS_COM == as.character(ss))
        hallmat &lt;- matrix(0,nrow = 4, ncol = 4);
        rownames(hallmat) &lt;- halls
        colnames(hallmat) &lt;- halls
    if(nrow(data2) &gt; 0) {
        for(i in 1:nrow(data2)){
        hallmat[as.character(data2$REF[i]),as.character(data2$ANS[i])] &lt;- hallmat[as.character(data2$REF[i]), as.character(data2$ANS[i])] + 1
        }
    }
    musmatsSS[[as.character(ss)]] &lt;- hallmat
    }
musmatsS[[as.character(s)]] &lt;- musmatsSS
}

# Let&#39;s see what we have:
hallmat1 # overall same</code></pre>
<pre><code>##    AC BP CP MH
## AC 23 16 18 23
## BP 15 26 23 16
## CP 17 24 19 20
## MH 21 20 24 15</code></pre>
<pre class="r"><code>#hallmat2 # overall diff (not shown)
#samemats # per music, same (not shown)
#diffmats # per music, diff (not shown)
#musmatsS # per music vs music (not shown)</code></pre>
</div>
<div id="analysis-with-caret-package" class="section level3">
<h3>Analysis with caret-package</h3>
<p>Now that we have our set of main confusion matrices ready (we won’t be looking at individual level performance here), there a various R packages that can be used for the analysis. Here, we will be using the caret -package, and as an example, we will analyse only a single matrix.</p>
<pre class="r"><code>library(caret) # caret-pkg</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre class="r"><code>library(e1071) # this is required in rstudio
# OVERALL SAME:
confumat &lt;- hallmat1 # assign to confumat 

# For using caret confusionMatrix-function we need to spell out the reference and response table in this way:
Reference &lt;- factor(rep(halls, times = c(sum(confumat[1,]),sum(confumat[2,]),sum(confumat[3,]),sum(confumat[4,]))), levels = halls)

Response &lt;- factor(
c(
rep(halls, times = c(confumat[1,1], confumat[1,2], confumat[1,3], confumat[1,4])),
rep(halls, times = c(confumat[2,1], confumat[2,2], confumat[2,3], confumat[2,4])),
rep(halls, times = c(confumat[3,1], confumat[3,2], confumat[3,3], confumat[3,4])),
rep(halls, times = c(confumat[4,1], confumat[4,2], confumat[4,3], confumat[4,4]))),
levels = halls
)

# OBS: caret::confusionMatrix -function will need the data so that responses (predictions) are in the order table(Response,Reference), so responses in rows and references in cols. (Opposite of our experiment)
xtab &lt;- table(Response,Reference)</code></pre>
<p>And now we are ready to run the analysis function from the caret-pckg:</p>
<pre class="r"><code>#### ANALYSIS:
confusionMatrix(xtab, mode = &quot;everything&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##         Reference
## Response AC BP CP MH
##       AC 23 15 17 21
##       BP 16 26 24 20
##       CP 18 23 19 24
##       MH 23 16 20 15
## 
## Overall Statistics
##                                           
##                Accuracy : 0.2594          
##                  95% CI : (0.2122, 0.3111)
##     No Information Rate : 0.25            
##     P-Value [Acc &gt; NIR] : 0.3698          
##                                           
##                   Kappa : 0.0125          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.9863          
## 
## Statistics by Class:
## 
##                      Class: AC Class: BP Class: CP Class: MH
## Sensitivity            0.28750   0.32500   0.23750   0.18750
## Specificity            0.77917   0.75000   0.72917   0.75417
## Pos Pred Value         0.30263   0.30233   0.22619   0.20270
## Neg Pred Value         0.76639   0.76923   0.74153   0.73577
## Precision              0.30263   0.30233   0.22619   0.20270
## Recall                 0.28750   0.32500   0.23750   0.18750
## F1                     0.29487   0.31325   0.23171   0.19481
## Prevalence             0.25000   0.25000   0.25000   0.25000
## Detection Rate         0.07187   0.08125   0.05937   0.04688
## Detection Prevalence   0.23750   0.26875   0.26250   0.23125
## Balanced Accuracy      0.53333   0.53750   0.48333   0.47083</code></pre>
<p>As shown, the confusionMatrix()-function outputs a set of performance metrics, and information on these metrics can be found from the <a href="https://en.wikipedia.org/wiki/Confusion_matrix">wikipedia page</a>. Also give a look <a href="https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2">here</a> and <a href="https://www.datascienceblog.net/post/machine-learning/performance-measures-multi-class-problems/">here</a> for some more tutorials on multiclass classification</p>
<p>Here is a very short summary about the terminology and the derivations:</p>
<ul>
<li>condition positive (<strong>P</strong>) : the number of real positive cases in the data</li>
<li>condition negative (<strong>N</strong>): the number of real negative cases in the data</li>
<li>true positive (<strong>TP</strong>), items that are correctly classified, i.e., “hit”</li>
<li>true negative (<strong>TN</strong>), items that are correctly classified as not belonging to the class, i.e. correct rejection</li>
<li>false positive (<strong>FP</strong>), items that are incorrectly perceived to belong to the class, i.e., false alarm (Type I error)</li>
<li>false negative (<strong>FN</strong>), items that are not perceived as belonging to the class but should have been; i.e., “miss” (Type II error)</li>
</ul>
<p>The main metrics that may be interested in:</p>
<ul>
<li><p><strong>Accuracy</strong>: The overall accuracy of the prediction (TP + TN) / P + N</p></li>
<li><p><strong>Recall</strong>; <strong>sensitivity</strong>; <strong>hit rate</strong>; <strong>true positive rate</strong> : the proportion of correct answers to total answers TP / P = TP / (TP + FN)</p></li>
<li><p><strong>Precision</strong> is the proportion of predictions that are correct from all “positive” predictions of that class TP / (TP + FP)</p></li>
</ul>
<p>Recall and precision (and other metrics) can be calculated the following table that is generated separately for each hall :</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Reference</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Prediction</strong></td>
<td>Hall 1</td>
<td>Hall 234</td>
</tr>
<tr class="even">
<td>Hall 1</td>
<td>A (TP)</td>
<td>B (FP)</td>
</tr>
<tr class="odd">
<td>Hall 234</td>
<td>C (FN)</td>
<td>D (TN)</td>
</tr>
</tbody>
</table>
<p>And for your reference (from caret-pckg documentation)</p>
<ul>
<li>Sensitivity = A/(A+C)</li>
<li>Specificity = D/(B+D)</li>
<li>Prevalence = (A+C)/(A+B+C+D)</li>
<li>Positive Predictive Value (PPV) (same as Precision) = (sensitivity * prevalence)/((sensitivity*prevalence) + ((1-specificity)*(1-prevalence)))</li>
<li>Negative Preditive Value (NPV) = (specificity * (1-prevalence))/(((1-sensitivity)<em>prevalence) +
((specificity)</em>(1-prevalence)))</li>
<li>Detection Rate = A/(A+B+C+D)</li>
<li>Detection Prevalence = (A+B)/(A+B+C+D)</li>
<li>Balanced Accuracy = (sensitivity+specificity)/2</li>
</ul>
<p>The output includes <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen’s <strong>Kappa</strong> statistic</a> which measures the agreement between the predictions and the reference counts and it ranges from 0 to 1, so that, close to zero values indicate that the performance is not better than change level.</p>
<p>It is interesting that also P-value of <a href="https://en.wikipedia.org/wiki/McNemar%27s_test">McNemar’s test</a> is included. McNemar’s test indicates whether the marginal frequecies in the confusion matrix are equal (null hypothesis) or not (alternative hypothesis). This test requires matched pairs of observations, and I do not fully understand what is the meaning here.</p>
<p>However, McNemar’s test is actually quite useful in many cases when one can form matched pairs from the data, and for our current analysis, there is actually another use of McNemar’s test illustrated next.</p>
<p>We can generate a contingengy table between the “same” and “different” sounds from our data by forming matched pairs where the same subject has made task with a particular sound and a particular concert hall as the reference sample. The following script illustrates what I mean:</p>
<pre class="r"><code># McNemar&#39;s test; create contingengy table between same and different by pairing the answers

# case by case
A &lt;- 0 # number of correct answers on both same and different
B &lt;- 0 # number of answers where same is correct and different is incorrect
C &lt;- 0 # number of answers where same is incorrect and different is correct
D &lt;- 0 # number of incorrect answers on both same and different

musref &lt;- levels(simdata$MUS_REF)
halls &lt;- levels(simdata$REF)
subs &lt;- levels(simdata$SUB_ID)

for(m in musref){
    for (h in halls){       
        sametemp &lt;- subset(simdata, REF == h &amp; MUS_REF == m &amp; SAMEDIFF == &#39;same&#39;)
        difftemp &lt;- subset(simdata, REF == h &amp; MUS_REF == m &amp; SAMEDIFF == &#39;diff&#39;)
    
        for (s in subs){
            if (sametemp[sametemp$SUB_ID == s,]$CORRECT == 1 &amp;&amp; difftemp[difftemp$SUB_ID == s,]$CORRECT == 1) {
                A &lt;- A + 1;
            }
            if (sametemp[sametemp$SUB_ID == s,]$CORRECT == 1 &amp;&amp; difftemp[difftemp$SUB_ID == s,]$CORRECT == 0){
                B &lt;- B + 1
            }
            if (sametemp[sametemp$SUB_ID == s,]$CORRECT == 0 &amp;&amp; difftemp[difftemp$SUB_ID == s,]$CORRECT == 1){
                C &lt;- C + 1
            }
            if (sametemp[sametemp$SUB_ID == s,]$CORRECT == 0 &amp;&amp; difftemp[difftemp$SUB_ID == s,]$CORRECT == 0){
                D &lt;- D + 1
            }
            
        }
    }
}

# CHECK THE RESULTS:
c(A, B, C, D)</code></pre>
<pre><code>## [1]  13  45  49 133</code></pre>
<pre class="r"><code># Perform the McNemar&#39;s test
Matching &lt;-
matrix(c(A, C, B, D),
       nrow = 2,
       dimnames = list(&quot;Same&quot; = c(&quot;Correct&quot;, &quot;Incorrect&quot;),
                       &quot;Diff&quot; = c(&quot;Correct&quot;, &quot;Incorrect&quot;)))
Matching</code></pre>
<pre><code>##            Diff
## Same        Correct Incorrect
##   Correct        13        45
##   Incorrect      49       133</code></pre>
<pre class="r"><code>mcnemar.test(Matching)</code></pre>
<pre><code>## 
##  McNemar&#39;s Chi-squared test with continuity correction
## 
## data:  Matching
## McNemar&#39;s chi-squared = 0.095745, df = 1, p-value = 0.757</code></pre>
<p>In this way, McNemar’s test provides as a nice direct test statistic in terms of our question whether there is a difference between between our “same” and “different” cases.</p>
<p>All together the analysis of this matching data (when it is also looked at different levels) seems now to be quite exhaustive.</p>
<p>…</p>
<p>Finally, in order to have some more figures to accompany the analysis here is an example of how to illustrate the results by plotting the confusion matrices with ggplot2. (in the following, xtab2 was generated the same way as xtab.)</p>
<pre class="r"><code>### PLOTTING WITH GGPLOT
# SAME plot
normvalue &lt;- sum(hallmat1[1,]) # normalize
melted_xtab &lt;- melt(round(xtab1/normvalue,2))
gsame &lt;- ggplot(data = melted_xtab, aes(x=Response, y=Reference, fill=value),show.legend = F) + geom_tile(show.legend = F) + theme_minimal() +
geom_text(aes(Response, Reference, label = value), color = &quot;black&quot;, size = 3,show.legend = F) + ggtitle(&#39;Same&#39;)

# DIFF Plot
normvalue &lt;- sum(hallmat2[1,])
melted_xtab &lt;- melt(round(xtab2/normvalue,2))
gdiff &lt;- ggplot(data = melted_xtab, aes(x=Response, y=Reference, fill=value),show.legend = F) + geom_tile(show.legend = F) + theme_minimal() +
geom_text(aes(Response, Reference, label = value), color = &quot;black&quot;, size = 3,show.legend = F) +ggtitle(&#39;Different&#39;)
#gdiff

grid.arrange(gsame,gdiff,nrow=1)</code></pre>
<p><img src="../../../post/2020-22-01-chm/2020-22-01-chm_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>So there we are! And we will stop for now with this simulated dataset.</p>
</div>
<div id="next-steps.." class="section level3">
<h3>Next steps..</h3>
<p>Next steps would be to analyse the dataset in detail, investigate the differences between same and different excitation signals, evaluate whether there are some particular patterns emerging between the halls, and finally to provide an answer to the original question(s) about the human ability to match concert halls when listening to the same and different excitation signals?</p>
<p>If all goes as planned you’ll find the answer in a proper article sometime in the future… and here it is <a href="https://asa.scitation.org/doi/10.1121/10.0001915">“Recognizing individual concert halls is difficult when listening to the acoustics with different musical passages”</a>.</p>
<p>Also see the next <a href="https://anttikuusinen.github.io/post/2021-04-01-onlinematching/2021-04-01-onlinematching/">post</a> about the results of the online listening experiment.</p>
<hr />
<p>Leave a comment:</p>
<script src="https://utteranc.es/client.js"
        repo="anttikuusinen/anttikuusinen.github.io"
        issue-term="[CHMdata]"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>
</div>
</div>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="../../../tags/r-markdown/">R Markdown</a>
  
  <a class="badge badge-light" href="../../../tags/data-preprocessing/">data preprocessing</a>
  
  <a class="badge badge-light" href="../../../tags/confusion-matrix/">confusion matrix</a>
  
  <a class="badge badge-light" href="../../../tags/classification/">classification</a>
  
</div>














  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="../../../authors/admin/avatar_hu5eefcd357d44e1910a828a3ff6ecb76d_370497_250x250_fill_q90_lanczos_center.JPG" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://anttikuusinen.github.io/">Antti Kuusinen</a></h5>
      <h6 class="card-subtitle">Postdoctoral Researcher</h6>
      <p class="card-text">My research interests include the human perception of speech, music and noise in reverberant spaces.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:antti.kuusinen@aalto.fi" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/anttikuusinen/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.fi/citations?user=HJkdqi8AAAAJ&amp;hl=fi" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Antti_Kuusinen" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/anttikuusinen/" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/0000-0002-8106-0421" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  



  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

      
      
    

    
    
      <script async defer src="https://maps.googleapis.com/maps/api/js?key="></script>
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/gmaps.js/0.4.25/gmaps.min.js" integrity="sha256-7vjlAeb8OaTrCXZkCNun9djzuB2owUsaO72kXaFDBJs=" crossorigin="anonymous"></script>
      
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="../../../js/academic.min.d6bd04fdad2ad213aa8111c5a3b72fc5.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    ©2021 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
